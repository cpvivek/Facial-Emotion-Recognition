{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_DL&MLE_Facial_Emotion_Recognition_Vivek_CP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpvivek/Facial-Emotion-Recogonition/blob/main/Capstone_DL%26MLE_Facial_Emotion_Recognition_Vivek_CP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_adQ5zbi5U3Z"
      },
      "source": [
        "# Facial Emotion Detection\n",
        "In this project, you will implement a CNN model from scratch using a Deep Learning library Tensorflow. The model will be trained to detect facial emotions of humans via live camera feed.\n",
        "\n",
        "If you don't have much experience with OpenCV or CNNs, don't worry about it, we have shared links to learn some basics about them as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISCJU9If5X4U"
      },
      "source": [
        "## Prerequisites\n",
        "- **Python**\n",
        "- **Deep Learning Library (Tensorflow)**\n",
        "- **OpenCV** : OpenCV is a huge open-source library for computer vision, machine learning, and image processing. Only the basics of OpenCV would be required for this project.\n",
        "- **VideoGuide** : Go through the dashboard for the link.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7a29k1g6P20"
      },
      "source": [
        "## Pathway\n",
        "- First we need a dataset. Download it from here: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data?select=fer2013.tar.gz  \n",
        "- Split the data into training and validation sets, this is can be done in a ratio of about 10:1 or less\n",
        "- Now augment the image data\n",
        "- Now, code a CNN model, based on what you learnt from the link in prerequisites. A block would contain a Conv2D layer, Activation layer, BatchNorm, MaxPool, Dropout(optional). Create several such blocks (with successive blocks having double filters in Conv2D layer). Finally include a Flatten Layer, Dense layer, Activation layer. This, of course, is just a blueprint to give you direction. We'd encourage to try out different parameters and tinker around with the model to get some more practical knowledge.\n",
        "- Now come the standard training, testing, and hyperparameter training. After this, you can save the model to be used to create the emotion detector.\n",
        "- Now we'll detect faces using OpenCV. It's pretty simple actually. Read up on Haar Cascade classifier here: https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html, it's based on the famous Viola-Jones algorithm. You can use this directly in OpenCV to detect faces in livestream.\n",
        "- Select the RoI and use your model to classify the expression. You can make a bounding box and put text on it as well.\n",
        "- Done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5auocjrAaCL"
      },
      "source": [
        "## Learning Outcome\n",
        "- Convolutional Neural Networks using Tensorflow Keras\n",
        "- Data Augmentation\n",
        "- Hyperparameter Tuning\n",
        "- Applications of OpenCV for Building Face Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxdkaQg_9Eab"
      },
      "source": [
        "## Solution Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTagtqGLoXLd"
      },
      "source": [
        "import necessary libraries including tensorflow, keras, sklearn and opencv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I200zV42T-ye"
      },
      "source": [
        "# import necessary files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH3rfnChvO5m"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72-k5mJMofof"
      },
      "source": [
        "Importing dataset sourced from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dGzzjaX_ynd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4b68da-17cf-4b3a-8c6a-b5b2bcd2adb9"
      },
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VawKE3CGomDk"
      },
      "source": [
        "reading the csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FDaaif4sb2p"
      },
      "source": [
        "file_path='/content/drive/MyDrive/Alma Better Pro/Alma Better Pro Program/Module 4: Machine Learning/Data Sets/icml_face_data.csv'\n",
        "\n",
        "df=pd.read_csv(file_path) #facial emotion data stored as a dataframe in fe_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into our dataset"
      ],
      "metadata": {
        "id": "VQUjjIU6wDbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuJoqvJ3wIV0",
        "outputId": "8ed2575d-5441-442c-cae0-b29fab5009a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['emotion', ' Usage', ' pixels'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the dataset is not all that complicated.\n",
        "\n",
        "We've got 'emotion' field indicating different emotions.\n",
        "\n",
        "I believe since this is a dataset used in kaggle competitions, they've went ahead and done the training, validation and test segregation.\n",
        "\n",
        "And finally in the pixels field, we have the pixelated form of the image, flattened into an 1 dimension array. "
      ],
      "metadata": {
        "id": "l0v6Ru-BwKYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu_6lrbYv93R",
        "outputId": "eccdbecf-f07d-4150-879c-afd306325c5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1    Usage   35887 non-null  object\n",
            " 2    pixels  35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixel field is stored as a string at the moment. We'll have to convert that into a 48x48x1 array with float values."
      ],
      "metadata": {
        "id": "SQQXxPdgwwTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using lambda function to achieve this conversion\n",
        "df[' pixels']=df[' pixels'].apply(lambda x: np.fromstring(x, sep=' ',dtype='float32')) #converting string to float separated by ' ' \n",
        "df[' pixels']=df[' pixels'].apply(lambda x:np.asarray(x.reshape(48,48,1))) #reshaping to 48x48x1"
      ],
      "metadata": {
        "id": "iR7QUDPLiqgH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into emotions field."
      ],
      "metadata": {
        "id": "27sj2NJR_EE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['emotion'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFYx4zsX_Bsu",
        "outputId": "fce2bc62-3a1e-4816-ee0e-0c2b976041f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 4, 6, 3, 5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 7 different emotions labeled using number from 0 to 6."
      ],
      "metadata": {
        "id": "U7QM5p3bb_ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's segregate our training, validation and test sets right away. \n",
        "\n",
        "Public Test would be put into validation and Private test into test set. "
      ],
      "metadata": {
        "id": "jobLtXfu_2Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defininng train, val and test data.\n",
        "training_data=df[df[' Usage']=='Training']\n",
        "validation_data=df[df[' Usage']=='PublicTest']\n",
        "test_data=df[df[' Usage']=='PrivateTest']"
      ],
      "metadata": {
        "id": "cv6UTPyhAtTP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data['emotion'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnVhkAlVBV8m",
        "outputId": "b36f444b-e1a3-426c-f37c-b3ff142dfe8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    7215\n",
              "6    4965\n",
              "4    4830\n",
              "2    4097\n",
              "0    3995\n",
              "5    3171\n",
              "1     436\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is considerable amount of imbalance in the dataset wrt to emotions. This will create bias in the model and result in misclassifying. \n",
        "\n",
        "We can treat this issue by data augmentation."
      ],
      "metadata": {
        "id": "1EqSHNiPccDC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3SouBFAovQm"
      },
      "source": [
        "Using keras for data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5Hd0XZW2_WA"
      },
      "source": [
        "# Data Augmentation\n",
        "from keras import layers\n",
        "data_augmentation=keras.Sequential(\n",
        "    [\n",
        "     layers.RandomFlip('horizontal'), #introduce some random flips on horizontal axis\n",
        "     layers.RandomRotation(0.015), #introduce some rotation by 0.015 degree\n",
        "     layers.RandomZoom(0.15,0.15) #introduce some random zoom\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6chXbR7BpOD-"
      },
      "source": [
        "Applying Data Augmentation on training samples so that each emotion sample has equal number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mlr7tSks09Y"
      },
      "source": [
        "aug_df=pd.DataFrame({}) #creating a neww dataframe that will contain the augmented images\n",
        "emotion=[] #list for emotion labels\n",
        "aug_pixels=[] #list for augemnted image pixels\n",
        "sample_length=7500 #specifying desired number of samples for each emotions. arbitrary choice\n",
        "\n",
        "for i in range(7): #since 7 emotions\n",
        "  data=training_data[training_data['emotion']==i] #selecting the image to be augmented\n",
        "  j=sample_length - len(data) #calculating number of samples required to reach 7500\n",
        "  for k in range(j):\n",
        "    ind= k%len(data) #setting index\n",
        "    augmented_image=data_augmentation(data[' pixels'][data.index[ind]]) #augmenting images\n",
        "    aug_pixels.append(augmented_image) #appending augmented image to \n",
        "    emotion.append(i)#appending emotion label\n",
        "\n",
        "aug_df[' pixels'] = aug_pixels #column containing augmented pixels\n",
        "aug_df['emotion']= emotion#corresponding emotion label"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8XmdhG4crYN",
        "outputId": "9d8a5500-48e3-4507-b657-458707dbe332"
      },
      "source": [
        "#count of augmented images that will be added for each emotions\n",
        "aug_df['emotion'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    7064\n",
              "5    4329\n",
              "0    3505\n",
              "2    3403\n",
              "4    2670\n",
              "6    2535\n",
              "3     285\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHQF13CIbG4e",
        "outputId": "c7a325fe-7651-4655-b286-2022802b0dc5"
      },
      "source": [
        "# concat train data and augmented df\n",
        "training_data=pd.concat([training_data,aug_df],axis=0)\n",
        "training_data['emotion'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    7500\n",
              "2    7500\n",
              "4    7500\n",
              "6    7500\n",
              "3    7500\n",
              "5    7500\n",
              "1    7500\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Up08GIphoB"
      },
      "source": [
        "Since augmented data and training data index wouldn't matchup, we need set up proper index for the new training dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5QhByxIThklb",
        "outputId": "0a0dd1c7-a4c0-41bc-dba7-c216121ba134"
      },
      "source": [
        "ind=[ind for ind in range(len(training_data))]\n",
        "training_data.index=ind\n",
        "training_data.tail()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       emotion  Usage                                             pixels\n",
              "52495        6    NaN  (((tf.Tensor(68.40772, shape=(), dtype=float32...\n",
              "52496        6    NaN  (((tf.Tensor(86.45305, shape=(), dtype=float32...\n",
              "52497        6    NaN  (((tf.Tensor(141.01285, shape=(), dtype=float3...\n",
              "52498        6    NaN  (((tf.Tensor(198.45676, shape=(), dtype=float3...\n",
              "52499        6    NaN  (((tf.Tensor(166.83704, shape=(), dtype=float3..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2841ba9-5a74-4b47-bb90-bc5234a7d391\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>Usage</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52495</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(((tf.Tensor(68.40772, shape=(), dtype=float32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52496</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(((tf.Tensor(86.45305, shape=(), dtype=float32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52497</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(((tf.Tensor(141.01285, shape=(), dtype=float3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52498</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(((tf.Tensor(198.45676, shape=(), dtype=float3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52499</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(((tf.Tensor(166.83704, shape=(), dtype=float3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2841ba9-5a74-4b47-bb90-bc5234a7d391')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2841ba9-5a74-4b47-bb90-bc5234a7d391 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2841ba9-5a74-4b47-bb90-bc5234a7d391');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have 52499 datapoints now. Ignore the NaN values in usage. We know the usage is training"
      ],
      "metadata": {
        "id": "trdHWG5p2FnM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN8crTUqpoOJ"
      },
      "source": [
        "Preprocessing X_train using keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlKUs7Dgo-iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420a7f72-c4fb-4901-84a1-e0c8d02dacd8"
      },
      "source": [
        "x_train=[]\n",
        "for i in training_data[' pixels']:\n",
        "  x_train.append(i)\n",
        "x_train=np.asarray(x_train)\n",
        "x_train=x_train.reshape(len(x_train),48,48,1)\n",
        "y_train=np.array(training_data['emotion'])\n",
        "y_train=y_train.astype(int)\n",
        "y_train=np_utils.to_categorical(y_train,7)\n",
        "# shape of training data\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52500, 48, 48, 1), (52500, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBw-ql-BpuFm"
      },
      "source": [
        "Preprocessing X_val using keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQ-pAHq9zuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e581872c-4884-4ec7-b8cd-7f10bc4d64e5"
      },
      "source": [
        "x_val = []\n",
        "for i in validation_data[' pixels']:\n",
        "  x_val.append(i)\n",
        "x_val=np.asarray(x_val)\n",
        "x_val= x_val.reshape(len(x_val),48,48,1)\n",
        "y_val=np.array(validation_data['emotion'])\n",
        "y_val=y_val.astype(int)\n",
        "y_val=np_utils.to_categorical(y_val,7)\n",
        "\n",
        "\n",
        "\n",
        "x_val.shape,y_val.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3589, 48, 48, 1), (3589, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ggjy26zpwU7"
      },
      "source": [
        "Preprocessing X_val using keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC60Rvk8v4Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c524fb17-3b76-4ed0-fe4e-74c1de5023b6"
      },
      "source": [
        "x_test = []\n",
        "\n",
        "for in testing_data[' pixels']:\n",
        "  x_test.append(i)\n",
        "x_test=np.array(x_test)\n",
        "x_test=x_test.reshape(len(x_test)48,48,1)\n",
        "y_test=np.array(test_data['emotion'])\n",
        "y_test=y_test.astype(int)\n",
        "y_test=np_utile.to_catoegorical(y_test,7)\n",
        "# code here\n",
        "\n",
        "# then check shape\n",
        "X_test.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3589, 48, 48, 1), (3589, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}